<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhiyuan You (Â∞§ÂøóËøú)</title>

  <meta name="author" content="Zhiyuan You">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zhiyuan You (Â∞§ÂøóËøú)</name>
                  </p>
                  <p>I am a first-year Ph.D. student of the joint program between
                    <a href="https://mmlab.ie.cuhk.edu.hk/">MMLab</a>, The Chinese University of Hong Kong
                    (<a href="https://cuhk.edu.hk/english/index.html">CUHK</a>) and
                    <a href="https://mmlab.siat.ac.cn/">MMLab</a>,
                    Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences
                    (<a href="https://english.siat.ac.cn/">SIAT</a>).
                    I am supervised by Prof. <a href="https://xpixel.group/2010/01/20/chaodong.html">Chao Dong</a>
                    and Prof. <a href="https://tianfan.info/">Tianfan Xue</a>.
                    I also work closely with Dr. <a href="https://www.jasongt.com/">Jinjin Gu</a>.
                  </p>
                  <p>
                    Prior to that, I received my M.Eng. from Shanghai Jiao Tong University
                    (<a href="https://en.sjtu.edu.cn/">SJTU</a>) in 2023, supervised by
                    Prof. <a href="https://scholar.google.com/citations?user=MGZyMf4AAAAJ">Xinyi Le</a> and
                    Prof. <a href="https://me.sjtu.edu.cn/teacher_directory1/zhengyu.html">Yu Zheng</a>,
                    and my B.Eng. from the same university in 2020.
                    I spent a wonderful time in <a href="https://horizon.cc/">Horizon Robotics</a> and
                    <a href="https://www.sensetime.com/en">SenseTime</a> as a research intern, mentored by
                    Dr. <a href="https://scholar.google.com/citations?user=21NZzksAAAAJ">Yuelong Yu </a> and
                    Mr. <a href="https://scholar.google.com/citations?user=s5Z6X0cAAAAJ">Kai Yang</a>, respectively.
                  </p>
                  <p>
                    My current research interest mainly lies in <span class="highlight">multi-modality vision for
                    low-level vision, anomaly detection, few-shot learning, and industrial visual inspection</span>.
                  </p>
                  <p style="text-align:center">
                    <a href="data/CV_ZhiyuanYOU.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=Gwo9O8sAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/zhiyuanyou/">Github</a>
                    <br>
                    <a href="mailto:zhiyuanyou@foxmail.com">Email</a>: zhiyuanyou [at] foxmail [dot] com
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/zhiyuanyou.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/zhiyuanyou.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                    <strong>[2024.07]</strong>&nbsp;&nbsp;&nbsp;&nbsp;One paper to appear in ECCV 2024. See you in Milan. 
                  </p>
                  <p>
                    <strong>[2023.08]</strong>&nbsp;&nbsp;&nbsp;&nbsp;I become a Ph.D. student at MMLab in The Chinese University of Hong Kong.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    *: Equal Contribution, ‚Ä†: Corresponding Author
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

            <tr>
              <td style="padding-left:20px">
                <subheading>arXiv, preprint</subheading>
              </td>
            </tr>

            <tr onmouseout="fhnet_stop()" onmouseover="fhnet_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/depictqa-wild.png' style="width:100%">
                </div>
                <script type="text/javascript">
                  function fhnet_start() {
                    document.getElementById('fhnet_image').style.opacity = "1";
                  }
                  function fhnet_stop() {
                    document.getElementById('fhnet_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2405.18842">
                  <papertitle>Descriptive Image Quality Assessment in the Wild</papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Jinjin Gu, Zheyuan Li, Xin Cai, Kaiwen Zhu, Chao Dong‚Ä†, Tianfan Xue‚Ä†
                <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2405.18842">paper</a>
                /
                <a href="https://depictqa.github.io">project page</a>
                /
                <a href="https://github.com/XPixelGroup/DepictQA">code</a>
                <p></p>
                <p>We introduce <em>DepictQA-Wild</em>, a multi-functional in-the-wild descriptive image quality
                  assessment model.
                </p>
              </td>
            </tr>

            <tr onmouseout="fhnet_stop()" onmouseover="fhnet_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/maskma.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function fhnet_start() {
                    document.getElementById('fhnet_image').style.opacity = "1";
                  }
                  function fhnet_stop() {
                    document.getElementById('fhnet_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2310.11846">
                  <papertitle>
                    MaskMA: Towards Zero-Shot Multi-Agent Decision Making with Mask-Based Collaborative Learning
                  </papertitle>
                </a>
                <br>
                Jie Liu*, Yinmin Zhang*, Chuming Li, <strong>Zhiyuan You</strong>, 
                Zhanhui Zhou, Chao Yang, Yaodong Yang‚Ä†, Yu Liu, Wanli Ouyang
                <br>
                <em>arXiv</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2310.11846">paper</a>
                <p></p>
                <p>We release <em>MaskMA</em>, a masked pretraining framework for multi-agent decision-making. 
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding-left:20px">
                <br><subheading>Conference</subheading>
              </td>
            </tr>

            <tr onmouseout="fhnet_stop()" onmouseover="fhnet_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/depictqa.png' style="width:100%">
                </div>
                <script type="text/javascript">
                  function fhnet_start() {
                    document.getElementById('fhnet_image').style.opacity = "1";
                  }
                  function fhnet_stop() {
                    document.getElementById('fhnet_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.08962">
                  <papertitle>Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language
                    Models
                  </papertitle>
                </a>
                <br>
                <strong>Zhiyuan You*</strong>,
                Zheyuan Li*, Jinjin Gu*, Zhenfei Yin, Tianfan Xue‚Ä†, Chao Dong‚Ä†
                <br>
                European Conference on Computer Vision (<strong>ECCV</strong>), 2024
                <br>
                <a href="https://arxiv.org/abs/2312.08962">paper</a>
                /
                <a href="https://depictqa.github.io">project page</a>
                /
                <a href="https://github.com/XPixelGroup/DepictQA">code</a>
                <p></p>
                <p>We introduce <em>DepictQA</em>, leveraging Multi-modal Large Language Models, allowing for detailed,
                  language-based, and human-like evaluation of image quality.
                </p>
              </td>
            </tr>

            <tr onmouseout="cagroup_stop()" onmouseover="cagroup_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/safecount.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function cagroup_start() {
                    document.getElementById('cagroup_image').style.opacity = "1";
                  }
                  function cagroup_stop() {
                    document.getElementById('cagroup_image').style.opacity = "0";
                  }
                  cagroup_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2201.08959">
                  <papertitle>Few-shot Object Counting with Similarity-Aware Feature Enhancement</papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi Le‚Ä†
                <br>
                Winter Conference on Applications of Computer Vision (<strong>WACV</strong>), 2023, <strong>Oral</strong>
                <br>
                <a href="https://arxiv.org/abs/2201.08959">paper</a>
                /
                <a href="https://github.com/zhiyuanyou/SAFECount">code</a>
                /
                <a href="https://youtu.be/JzrCVyWujDY">video</a>
                <p></p>
                <p>
                  We propose a novel <em>SAFECount</em> block, equipped with a similarity comparison module and a
                  feature enhancement module for few-shot object counting.
                </p>
              </td>
            </tr>

            <tr onmouseout="fhnet_stop()" onmouseover="fhnet_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/uniad.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function fhnet_start() {
                    document.getElementById('fhnet_image').style.opacity = "1";
                  }
                  function fhnet_stop() {
                    document.getElementById('fhnet_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2206.03687">
                  <papertitle>A Unified Model for Multi-class Anomaly Detection
                  </papertitle>
                </a>
                <br>
                <strong>Zhiyuan You*</strong>,
                Lei Cui*, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, Xinyi Le‚Ä†
                <br>
                Neural Information Processing Systems (<strong>NeurIPS</strong>), 2022, <strong>Spotlight</strong>
                <br>
                <a href="https://arxiv.org/abs/2206.03687">paper</a>
                /
                <a href="https://github.com/zhiyuanyou/UniAD">code</a>
                <p></p>
                <p>We present <em>UniAD</em> that accomplishes anomaly detection for multiple classes with a unified
                  framework.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/adtr.jpg' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2209.01816">
                  <papertitle>ADTR: Anomaly Detection Transformer with Feature Reconstruction</papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Kai Yang, Wenhan Luo, Lei Cui, Yu Zheng, Xinyi Le‚Ä†
                <br>
                International Conference on Neural Information Processing (<strong>ICONIP</strong>), 2022, <strong>Oral</strong>
                <br>
                <a href="https://arxiv.org/abs/2209.01816">paper</a>
                <p></p>
                <p>We propose <em>ADTR</em> to apply a transformer to reconstruct pre-trained
                  features for anomaly detection, and propose novel losses to extend <em>ADTR</em> to
                  anomaly-available case (both image-level & pixel-level labeled).</p>
              </td>
            </tr>

            <tr>
              <td style="padding-left:20px">
                <br><subheading>Journal</subheading>
              </td>
            </tr>

            <tr onmouseout="mssvt_stop()" onmouseover="mssvt_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/utrad.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function mssvt_start() {
                    document.getElementById('mssvt_image').style.opacity = "1";
                  }
                  function mssvt_stop() {
                    document.getElementById('mssvt_image').style.opacity = "0";
                  }
                  mssvt_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608021004810">
                  <papertitle>UTRAD: Anomaly detection and localization with u-transformer</papertitle>
                </a>
                <br>
                Liyang Chen,
                <strong>Zhiyuan You</strong>,
                Nian Zhang,
                Juntong Xi,
                Xinyi Le‚Ä†
                <br>
                Neural Networks (<strong>NN</strong>), 2022
                <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608021004810">paper</a> /
                <a href="https://github.com/gordon-chenmo/UTRAD">code</a>
                <p></p>
                <p>We introduce <em>UTRAD</em>, a U-TRansformer based Anomaly Detection framework.
                </p>
              </td>
            </tr>

    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
          <heading>Education</heading>
        </td>
      </tr>
    </tbody>
  </table>
  <table width="100%" align="center" border="0" cellpadding="20">
    <tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cuhk.png" , width="80">
        </td>
        <td width="75%" valign="center">
          Ph.D. Student in Information Engineering @ The Chinese University of Hong Kong
          <br>
          Aug. 2023 - Current
          <br>
          Advisor: Prof. <a href="https://xpixel.group/2010/01/20/chaodong.html">Chao Dong</a> and
          Prof. <a href="https://tianfan.info/">Tianfan Xue</a>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sjtu.png" , width="80">
        </td>
        <td width="75%" valign="center">
          M.Eng. in Mechanical Engineering @ Shanghai Jiao Tong University
          <br>
          Sep. 2020 - Mar. 2023
          <br>
          Advisor: Prof. <a href="https://scholar.google.com/citations?user=MGZyMf4AAAAJ">Xinyi Le</a>
          and Prof. <a href="https://me.sjtu.edu.cn/teacher_directory1/zhengyu.html">Yu Zheng</a>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sjtu.png" , width="80"></td>
        <td width="75%" valign="center">
          B.Eng. in Mechanical Engineering @ Shanghai Jiao Tong University
          <br>
          Sep. 2016 - Jun. 2020
          <br>
          GPA: 3.78 / 4.0, Ranking: 5 / 148
        </td>
      </tr>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/horizon.png" , width="120">
            </td>
            <td width="75%" valign="center">
              <a href="https://horizon.cc/">Horizon Robotics</a>
              <br>
              Research Intern
              <br>
              Shanghai, China
              <br>
              Dec. 2022 - Mar. 2023
              <br>
              Mentor: Dr. <a href="https://scholar.google.com/citations?user=21NZzksAAAAJ">Yuelong
                Yu </a>
              <br>
              Perception Algorithm for Autonomous Driving
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sensetime.png" , width="120">
            </td>
            <td width="75%" valign="center">
              <a href="https://www.sensetime.com/en">SenseTime</a>
              <br>
              Research Intern
              <br>
              Shanghai, China
              <br>
              Dec. 2020 - Nov. 2022
              <br>
              Mentor: Mr. <a href="https://scholar.google.com/citations?user=s5Z6X0cAAAAJ">Kai Yang</a>
              <br>
              Anomaly Detection & Few-Shot Learning
            </td>
          </tr>

        </tbody>
      </table>

      <table style="width:33%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=1STUULBuvvkcU91CTePPxbN6eUqZhz9l5Q7wdwabKa0&cl=ffffff&w=a"></script>
        </tbody>
      </table>

      <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template from <a href="https://github.com/jonbarron/jonbarron_website">JonBarron</a>
              </p>
            </td>
          </tr>
        </tbody>
      </table>
      </td>
      </tr>
  </table>
</body>

</html>