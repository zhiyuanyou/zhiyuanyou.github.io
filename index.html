<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhiyuan You (Â∞§ÂøóËøú)</title>

  <meta name="author" content="Zhiyuan You">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zhiyuan You (Â∞§ÂøóËøú)</name>
                  </p>
                  <p>I am a second-year <s>first-year</s> Ph.D. student at Multimedia Laboratory 
                    (<a href="https://mmlab.ie.cuhk.edu.hk/">MMLab</a>), The Chinese University of Hong Kong
                    (<a href="https://cuhk.edu.hk/english/index.html">CUHK</a>).
                    I am supervised by Prof. <a href="https://xpixel.group/2010/01/20/chaodong.html">Chao Dong</a>
                    and Prof. <a href="https://tianfan.info/">Tianfan Xue</a>.
                    I also work closely with Dr. <a href="https://www.jasongt.com/">Jinjin Gu</a>.
                  </p>
                  <p>
                    Prior to that, I received my M.Eng. from Shanghai Jiao Tong University
                    (<a href="https://en.sjtu.edu.cn/">SJTU</a>) in 2023, supervised by
                    Prof. <a href="https://scholar.google.com/citations?user=MGZyMf4AAAAJ">Xinyi Le</a> and
                    Prof. <a href="https://me.sjtu.edu.cn/teacher_directory1/zhengyu.html">Yu Zheng</a>,
                    and my B.Eng. from the same university in 2020.
                    I spent a wonderful time in <a href="https://horizon.cc/">Horizon Robotics</a> and
                    <a href="https://www.sensetime.com/en">SenseTime</a> as a research intern, mentored by
                    Dr. <a href="https://scholar.google.com/citations?user=21NZzksAAAAJ">Yuelong Yu </a> and
                    Mr. <a href="https://scholar.google.com/citations?user=s5Z6X0cAAAAJ">Kai Yang</a>, respectively.
                  </p>
                  <p>
                    My current research interest mainly lies in <span class="highlight">multi-modality vision for
                    low-level vision, anomaly detection, few-shot learning, and industrial visual inspection</span>.
                  </p>
                  <p style="text-align:center">
                    <a href="data/CV_ZhiyuanYOU.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=Gwo9O8sAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/zhiyuanyou/">Github</a>
                    <br>
                    <a href="mailto:zhiyuanyou@foxmail.com">Email</a>: zhiyuanyou [at] foxmail [dot] com
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/zhiyuanyou.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/zhiyuanyou.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                  <strong>[2025.01]</strong>&nbsp;&nbsp;&nbsp;&nbsp;One paper to appear in ICLR 2025.<br>
                  <strong>[2024.12]</strong>&nbsp;&nbsp;&nbsp;&nbsp;One paper to appear in AAAI 2025.<br>
                  <strong>[2024.09]</strong>&nbsp;&nbsp;&nbsp;&nbsp;One paper to appear in NeurIPS 2024 (Spotlight).<br>
                  <strong>[2024.08]</strong>&nbsp;&nbsp;&nbsp;&nbsp;One paper to appear in TMLR.<br>
                  <strong>[2024.07]</strong>&nbsp;&nbsp;&nbsp;&nbsp;One paper to appear in ECCV 2024. See you in Milan.<br>
                  <strong>[2023.08]</strong>&nbsp;&nbsp;&nbsp;&nbsp;I become a Ph.D. student at MMLab in The Chinese University of Hong Kong.<br>
                  <strong>[2023.03]</strong>&nbsp;&nbsp;&nbsp;&nbsp;I graduate and receive my Master's degree from Shanghai Jiao Tong University.<br>
                  <strong>[2022.09]</strong>&nbsp;&nbsp;&nbsp;&nbsp;One paper to appear in NeurIPS 2022 (Spotlight).<br>
                  <strong>[2022.09]</strong>&nbsp;&nbsp;&nbsp;&nbsp;One paper to appear in ICONIP 2023 (Oral).<br>
                  <strong>[2022.08]</strong>&nbsp;&nbsp;&nbsp;&nbsp;One paper to appear in WACV 2023 (Early Accept).<br>
                  <strong>[2021.12]</strong>&nbsp;&nbsp;&nbsp;&nbsp;One paper to appear in NN.<br>
                  <strong>[2020.07]</strong>&nbsp;&nbsp;&nbsp;&nbsp;I graduate and receive my Bachelor's degree from Shanghai Jiao Tong University.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    *: Equal Contribution, ‚Ä†: Corresponding Author
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

              <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/deqa-score.png' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2501.11561">
                  <papertitle>Teaching Large Language Models to Regress Accurate Image Quality Scores using Score Distribution</papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Xin Cai, Jinjin Gu, Tianfan Xue‚Ä†, Chao Dong‚Ä†
                <br>
                <em>arXiv</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2501.11561">paper</a>
                /
                <a href="https://depictqa.github.io/deqa-score/">project page</a>
                /
                <a href="https://github.com/zhiyuanyou/DeQA-Score">code</a>
                /
                <a href="https://huggingface.co/datasets/zhiyuanyou/Data-DeQA-Score">data</a>
                <p></p>
                <p>We introduce <em>DeQA-Score</em>, a distribution-based depicted image quality assessment model 
                  for score regression.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/agenticir.png' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2410.17809">
                  <papertitle>An Intelligent Agentic System for Complex Image Restoration Problems</papertitle>
                </a>
                <br>
                Kaiwen Zhu, Jinjin Gu, <strong>Zhiyuan You</strong>, Yu Qiao, Chao Dong‚Ä†
                <br>
                International Conference on Learning Representations (<strong>ICLR</strong>), 2025
                <br>
                <a href="https://arxiv.org/abs/2410.17809">paper</a>
                /
                <a href="https://kaiwen-zhu.github.io/research/agenticir">project page</a>
                /
                <a href="https://github.com/Kaiwen-Zhu/AgenticIR">code</a>
                <p></p>
                <p>We introduce <em>AgenticIR</em>, a LLM-based agentic system that utilize various tools for complex 
                  image restoration problems.
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/sail.png' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2412.17092">
                  <papertitle>SAIL: Sample-Centric In-Context Learning for Document Information Extraction</papertitle>
                </a>
                <br>
                Jinyu Zhang*, <strong>Zhiyuan You</strong>*, Jize Wang, Xinyi Le‚Ä†
                <br>
                Association for the Advancement of Artificial Intelligence (<strong>AAAI</strong>), 2025
                <br>
                <a href="https://arxiv.org/abs/2412.17092">paper</a>
                /
                <a href="https://github.com/sky-goldfish/SAIL">code</a>
                <p></p>
                <p>We introduce <em>SAIL</em>, a sample-centric approach that selects tailored in-context examples 
                  for each test document.
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/depictqa-wild.png' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2405.18842">
                  <papertitle>Descriptive Image Quality Assessment in the Wild</papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Jinjin Gu, Zheyuan Li, Xin Cai, Kaiwen Zhu, Chao Dong‚Ä†, Tianfan Xue‚Ä†
                <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2405.18842">paper</a>
                /
                <a href="https://depictqa.github.io">project page</a>
                /
                <a href="https://github.com/XPixelGroup/DepictQA">code</a>
                <p></p>
                <p>We introduce <em>DepictQA-Wild</em>, a multi-functional in-the-wild descriptive image quality
                  assessment model.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/phocolens.png' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2409.17996">
                  <papertitle>PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging</papertitle>
                </a>
                <br>
                Xin Cai, <strong>Zhiyuan You</strong>, Hailong Zhang, Wentao Liu, Jinwei Gu, Tianfan Xue‚Ä†
                <br>
                Neural Information Processing Systems (<strong>NeurIPS</strong>), 2024, <strong>Spotlight</strong>
                <br>
                <a href="https://arxiv.org/abs/2409.17996">paper</a>
                /
                <a href="https://phocolens.github.io">project page</a>
                <p></p>
                <p>We introduce <em>PhoCoLens</em>, a novel two-stage approach for consistent and photorealistic lensless 
                  image reconstruction.
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/depictqa.png' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.08962">
                  <papertitle>Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language
                    Models
                  </papertitle>
                </a>
                <br>
                <strong>Zhiyuan You*</strong>,
                Zheyuan Li*, Jinjin Gu*, Zhenfei Yin, Tianfan Xue‚Ä†, Chao Dong‚Ä†
                <br>
                European Conference on Computer Vision (<strong>ECCV</strong>), 2024
                <br>
                <a href="https://arxiv.org/abs/2312.08962">paper</a>
                /
                <a href="https://depictqa.github.io">project page</a>
                /
                <a href="https://github.com/XPixelGroup/DepictQA">code</a>
                <p></p>
                <p>We introduce <em>DepictQA</em>, leveraging Multi-modal Large Language Models, allowing for detailed,
                  language-based, and human-like evaluation of image quality.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/maskma.jpg' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2310.11846">
                  <papertitle>
                    MaskMA: Towards Zero-Shot Multi-Agent Decision Making with Mask-Based Collaborative Learning
                  </papertitle>
                </a>
                <br>
                Jie Liu*, Yinmin Zhang*, Chuming Li, <strong>Zhiyuan You</strong>, 
                Zhanhui Zhou, Chao Yang, Yaodong Yang‚Ä†, Yu Liu, Wanli Ouyang
                <br>
                Transactions on Machine Learning Research (<strong>TMLR</strong>), 2024
                <br>
                <a href="https://arxiv.org/abs/2310.11846">paper</a>
                <p></p>
                <p>We release <em>MaskMA</em>, a masked pretraining framework for multi-agent decision-making. 
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/safecount.jpg' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2201.08959">
                  <papertitle>Few-shot Object Counting with Similarity-Aware Feature Enhancement</papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi Le‚Ä†
                <br>
                Winter Conference on Applications of Computer Vision (<strong>WACV</strong>), 2023, <strong>Early Accept</strong>
                <br>
                <a href="https://arxiv.org/abs/2201.08959">paper</a>
                /
                <a href="https://github.com/zhiyuanyou/SAFECount">code</a>
                /
                <a href="https://youtu.be/JzrCVyWujDY">video</a>
                <p></p>
                <p>
                  We propose a novel <em>SAFECount</em> block, equipped with a similarity comparison module and a
                  feature enhancement module for few-shot object counting.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/uniad.jpg' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2206.03687">
                  <papertitle>A Unified Model for Multi-class Anomaly Detection
                  </papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, Xinyi Le‚Ä†
                <br>
                Neural Information Processing Systems (<strong>NeurIPS</strong>), 2022, <strong>Spotlight</strong>
                <br>
                <a href="https://arxiv.org/abs/2206.03687">paper</a>
                /
                <a href="https://github.com/zhiyuanyou/UniAD">code</a>
                <p></p>
                <p>We present <em>UniAD</em> that accomplishes anomaly detection for multiple classes with a unified
                  framework.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/adtr.jpg' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2209.01816">
                  <papertitle>ADTR: Anomaly Detection Transformer with Feature Reconstruction</papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Kai Yang, Wenhan Luo, Lei Cui, Yu Zheng, Xinyi Le‚Ä†
                <br>
                International Conference on Neural Information Processing (<strong>ICONIP</strong>), 2022, <strong>Oral</strong>
                <br>
                <a href="https://arxiv.org/abs/2209.01816">paper</a>
                <p></p>
                <p>We propose <em>ADTR</em> to apply a transformer to reconstruct pre-trained
                  features for anomaly detection, and propose novel losses to extend <em>ADTR</em> to
                  anomaly-available case (both image-level & pixel-level labeled).</p>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/starid.png' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://link.springer.com/article/10.1007/s40747-021-00619-z">
                  <papertitle>An accurate star identification approach based on spectral graph matching for attitude measurement of spacecraft</papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Junzheng Li,
                Hongcheng Zhang,
                Bo Yang,
                Xinyi Le‚Ä†
                <br>
                Complex & Intelligent Systems (<strong>CAIS</strong>), 2022
                <br>
                <a href="https://link.springer.com/article/10.1007/s40747-021-00619-z">paper</a> /
                <a href="https://github.com/zhiyuanyou/star-ID-graph-match">code</a>
                <p></p>
                <p>My undergraduate thesis and my first first-author paper.</p>
                <p>We propose a novel star identification approach based on spectral graph matching.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/utrad.jpg' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608021004810">
                  <papertitle>UTRAD: Anomaly detection and localization with u-transformer</papertitle>
                </a>
                <br>
                Liyang Chen,
                <strong>Zhiyuan You</strong>,
                Nian Zhang,
                Juntong Xi,
                Xinyi Le‚Ä†
                <br>
                Neural Networks (<strong>NN</strong>), 2022
                <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608021004810">paper</a> /
                <a href="https://github.com/gordon-chenmo/UTRAD">code</a>
                <p></p>
                <p>We introduce <em>UTRAD</em>, a U-TRansformer based Anomaly Detection framework.
                </p>
              </td>
            </tr>

    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
          <heading>Education</heading>
        </td>
      </tr>
    </tbody>
  </table>
  <table width="100%" align="center" border="0" cellpadding="20">
    <tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cuhk.png" , width="80">
        </td>
        <td width="75%" valign="center">
          Ph.D. Student at Multimedia Laboratory (MMLab) @ The Chinese University of Hong Kong
          <br>
          Aug. 2023 - Current
          <br>
          Advisor: Prof. <a href="https://xpixel.group/2010/01/20/chaodong.html">Chao Dong</a> and
          Prof. <a href="https://tianfan.info/">Tianfan Xue</a>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sjtu.png" , width="80">
        </td>
        <td width="75%" valign="center">
          M.Eng. with Honor in Mechanical Engineering @ Shanghai Jiao Tong University
          <br>
          Sep. 2020 - Mar. 2023
          <br>
          GPA: 3.76 / 4.0
          <br>
          Advisor: Prof. <a href="https://scholar.google.com/citations?user=MGZyMf4AAAAJ">Xinyi Le</a>
          and Prof. <a href="https://me.sjtu.edu.cn/teacher_directory1/zhengyu.html">Yu Zheng</a>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sjtu.png" , width="80"></td>
        <td width="75%" valign="center">
          B.Eng. with Honor in Mechanical Engineering @ Shanghai Jiao Tong University
          <br>
          Sep. 2016 - Jun. 2020
          <br>
          GPA: 89.36 / 100, Ranking: 5 / 148
          <br>
          Advisor: Prof. <a href="https://scholar.google.com/citations?user=MGZyMf4AAAAJ">Xinyi Le</a>
        </td>
      </tr>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/horizon.png" , width="120">
            </td>
            <td width="75%" valign="center">
              <a href="https://horizon.cc/">Horizon Robotics</a>
              <br>
              Research Intern
              <br>
              Shanghai, China
              <br>
              Dec. 2022 - Mar. 2023
              <br>
              Mentor: Dr. <a href="https://scholar.google.com/citations?user=21NZzksAAAAJ">Yuelong
                Yu </a>
              <br>
              Perception Algorithm for Autonomous Driving
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sensetime.png" , width="120">
            </td>
            <td width="75%" valign="center">
              <a href="https://www.sensetime.com/en">SenseTime</a>
              <br>
              Research Intern
              <br>
              Shanghai, China
              <br>
              Dec. 2020 - Nov. 2022
              <br>
              Mentor: Mr. <a href="https://scholar.google.com/citations?user=s5Z6X0cAAAAJ">Kai Yang</a>
              <br>
              Anomaly Detection & Few-Shot Learning
            </td>
          </tr>

        </tbody>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Services</heading>
            <p><subheading>&bull; Journal Reviewer</subheading></p>
            <p>
              &nbsp;&nbsp;&nbsp;&nbsp;International Journal of Computer Vision (IJCV) <br>
              &nbsp;&nbsp;&nbsp;&nbsp;IEEE Transactions on Neural Networks and Learning Systems (T-NNLS) <br>
              &nbsp;&nbsp;&nbsp;&nbsp;IEEE Transactions on Systems, Man and Cybernetics: Systems (T-SMCS) <br>
              &nbsp;&nbsp;&nbsp;&nbsp;IEEE Transactions on Multimedia (T-MM) <br>
              &nbsp;&nbsp;&nbsp;&nbsp;IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT) <br>
              &nbsp;&nbsp;&nbsp;&nbsp;Pattern Recognition (PR) <br>
              &nbsp;&nbsp;&nbsp;&nbsp;Knowledge Based Systems (KBS) <br>
              &nbsp;&nbsp;&nbsp;&nbsp;Journal of Intelligent Manufacturing <br>
              &nbsp;&nbsp;&nbsp;&nbsp;Neurocomputing <br>
            </p>
            <p><subheading>&bull; Conference Reviewer</subheading></p>
            <p>
              &nbsp;&nbsp;&nbsp;&nbsp;IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, 2025 <br>
              &nbsp;&nbsp;&nbsp;&nbsp;Annual Conference on Neural Information Processing Systems (NeurIPS), 2024 <br>
              &nbsp;&nbsp;&nbsp;&nbsp;International Conference on Learning Representation (ICLR), 2025 <br>
              &nbsp;&nbsp;&nbsp;&nbsp;International Conference on Machine Learning (ICML), 2025 <br>
              &nbsp;&nbsp;&nbsp;&nbsp;International Conference on Artificial Intelligence and Statistics (AISTATS), 2025
            </p>
          </td>
        </tr>
      </tbody>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Awards</heading>
            <p>
              &nbsp;&nbsp;&nbsp;&nbsp;<strong>[2023.03]</strong>&nbsp;&nbsp;&nbsp;&nbsp;Excellent Master Dissertation <br>
              &nbsp;&nbsp;&nbsp;&nbsp;<strong>[2023.03]</strong>&nbsp;&nbsp;&nbsp;&nbsp;Outstanding Graduate (Postgraduate) <br>
              &nbsp;&nbsp;&nbsp;&nbsp;<strong>[2022.09]</strong>&nbsp;&nbsp;&nbsp;&nbsp;National Scholarship <br>
              &nbsp;&nbsp;&nbsp;&nbsp;<strong>[2020.06]</strong>&nbsp;&nbsp;&nbsp;&nbsp;Outstanding Graduate (Undergraduate)
            </p>
          </td>
        </tr>
      </tbody>
      </table>

      <table style="width:33%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=1STUULBuvvkcU91CTePPxbN6eUqZhz9l5Q7wdwabKa0'></script>
        </tbody>
      </table>

      <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template from <a href="https://github.com/jonbarron/jonbarron_website">JonBarron</a>
              </p>
            </td>
          </tr>
        </tbody>
      </table>
      </td>
      </tr>
  </table>
</body>

</html>